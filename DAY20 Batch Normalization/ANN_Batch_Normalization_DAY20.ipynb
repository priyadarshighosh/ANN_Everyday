{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "u93QB4H8gs0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Python libraries"
      ],
      "metadata": {
        "id": "vrEzW6nDrROu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9RMWnFZYq3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING MACHINE LEARNING LIBRARIES AND CLASSES"
      ],
      "metadata": {
        "id": "eQYW8ynycqpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from mlxtend.plotting import plot_decision_regions                   #for decision boundary\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score                 # for accuracy score\n",
        "from sklearn.model_selection import cross_val_score        # for cross validation score"
      ],
      "metadata": {
        "id": "5H7sUowSa3_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Deep learning Libraries"
      ],
      "metadata": {
        "id": "m0W9L8dyrAbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf                                                   #deep learning library\n",
        "\n",
        "from tensorflow import keras                                              # keras integrated with tf\n",
        "\n",
        "from tensorflow.keras import Sequential                                   #sequential model - arranging the Keras layers in a sequential order\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten                       # used to process information and simplify arrays in convolutional neural networks\n",
        "\n",
        "from tensorflow.keras.layers import Dropout                              #used to prevent overfitting\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam                             #Optimizer\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization                  #Batch Normalization\n"
      ],
      "metadata": {
        "id": "MJdstiajrA7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are making our own dataset"
      ],
      "metadata": {
        "id": "1BHUDrzIY0y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X , y =make_moons(n_samples=200, noise=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "nZMsdGPkY0Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0],X[:,1], c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxsVDNyVdAMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are not using Batch Normalization"
      ],
      "metadata": {
        "id": "8UCX5EW7cJ6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Dense(3, activation='relu',input_dim=2))\n",
        "model2.add(Dense(2, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "P1v8vwcPcJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "cWammWLNcJ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training The Model"
      ],
      "metadata": {
        "id": "n2gwHbITcyBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilation of the model"
      ],
      "metadata": {
        "id": "AqcPXBRPcyBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam=Adam(learning_rate=0.01)\n",
        "model2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WqZk8Ol9cyBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the model and getting the details"
      ],
      "metadata": {
        "id": "sFF0Y_J6cyBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history2=model2.fit(X,y, epochs=200,validation_split=0.2 ,verbose=0)"
      ],
      "metadata": {
        "id": "oXcKNRlgcyBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Graph !!"
      ],
      "metadata": {
        "id": "pIf4RZoHc8_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_regions(X, y.astype('int'),clf=model2, legend=2)\n",
        "plt.xlim(-2,3)\n",
        "plt.ylim(-2,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pWdZ4MOMc8_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and Prediction"
      ],
      "metadata": {
        "id": "hdAcNRlPc8_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['accuracy'])\n",
        "plt.plot(history2.history['val_accuracy'])\n",
        "\n",
        "plt.legend(['train', 'test'], loc='lower right')\n",
        "\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3qzeXimtc8_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history2.history['loss'])\n",
        "plt.plot(history2.history['val_loss'])\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ul4hnuiuc8_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUR Testing set giving an accuracy of 90perc which is good but we can do even better ."
      ],
      "metadata": {
        "id": "k9s0fu8GrmRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We are  using Batch Normalization"
      ],
      "metadata": {
        "id": "1rI4dbJZqoHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import M\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Dense(3, activation='relu',input_dim=2))\n",
        "model3.add(BatchNormalization())                          # THERE ARE 4 PARAMTERS FOR EACH NEURON THAT GAMMA , BETA AND MOVING MEAN AND MOVING STANDARD DEVIATION\n",
        "model3.add(Dense(2, activation='relu'))                #GAMMA AND BETA ARE TRAINABLE BUT MOVING AVERAGES OF MEAN AND STANDAR DEVIATION IS NOT TRAINABLE\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Ls_aSktEqoHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model3.summary()"
      ],
      "metadata": {
        "id": "FA-DNRz3qoHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training The Model"
      ],
      "metadata": {
        "id": "0vMBOlz9lllQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilation of the model"
      ],
      "metadata": {
        "id": "dScFoRgWllld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adam=Adam(learning_rate=0.01)\n",
        "model3.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DlQXZArhllld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the model and getting the details"
      ],
      "metadata": {
        "id": "56RVp7_vllle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history3=model3.fit(X,y, epochs=200,validation_split=0.2 ,verbose=1)"
      ],
      "metadata": {
        "id": "aPzX5KIgllle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Graph !!"
      ],
      "metadata": {
        "id": "oGXeyhfJlpWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_regions(X, y.astype('int'),clf=model3, legend=2)\n",
        "plt.xlim(-2,3)\n",
        "plt.ylim(-2,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PbEGagZnlpWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and Prediction"
      ],
      "metadata": {
        "id": "iO2vELP7lpWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history3.history['accuracy'])\n",
        "plt.plot(history3.history['val_accuracy'])\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mla-Jt18lpWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history3.history['loss'])\n",
        "plt.plot(history3.history['val_loss'])\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pPc4j_AilpWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUR result didnot come as we expected , atleast we learnt something new , that we can apply to future datasets ."
      ],
      "metadata": {
        "id": "ikf_tu_GszP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 thing er can take from here is -- Due to the batch normalization we come conclusion faster\n",
        "\n",
        "For Non- Batch normalized data it took 25 epochs to reach around 85 perc level but here with Batch normalized method we reached there in a few epochs."
      ],
      "metadata": {
        "id": "HZe1CVyRtzY8"
      }
    }
  ]
}