{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB7W9XF+oNjGHoIVrgx8u6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadarshighosh/ANN_Everyday/blob/main/ANN_Batch_vs_Stochastic_DAY11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATCH GRADIENT DESCENT VS STOCHASTIC GRADIENT DESCENT"
      ],
      "metadata": {
        "id": "buV9E0bf-37g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING THE DATA SCIENCE LIBRARIES"
      ],
      "metadata": {
        "id": "HNvHInaq_BMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eum2WGOnBeeL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the ML / Scikit learn EVERYTHING"
      ],
      "metadata": {
        "id": "egI1aKHM_G4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "from sklearn.pipeline import Pipeline,make_pipeline                    #here we wont use pipelines\n",
        "from sklearn.feature_selection import SelectKBest,chi2                 #feature selection                     #standard scaling\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "v-A2uAybzaKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing and CLEANING"
      ],
      "metadata": {
        "id": "ADdQQGes_Sdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "GynCwG0YCHzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Social_Network_Ads.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5btQS0GnzulN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape   #checking the shape of the data"
      ],
      "metadata": {
        "id": "uYYUMmaXDB6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()  #checking the info of the data"
      ],
      "metadata": {
        "id": "CTeAHSkCDMTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()  #checking the statistical description of the data"
      ],
      "metadata": {
        "id": "1Ik4DxWTDONQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['Age','EstimatedSalary','Purchased']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jtbiihl68bfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Purchased'])\n",
        "y = df['Purchased']"
      ],
      "metadata": {
        "id": "HBzF5maf0D6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "Ra4dvdOP0K3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "8KEzjeWN0LpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling ( STANDARD SCALING )"
      ],
      "metadata": {
        "id": "b6emVYzL_hrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "o0bDrSmD0_5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN  / TEST SPLIt"
      ],
      "metadata": {
        "id": "ErFOYn0X_iFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "LQ8WlW0P0dHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "zKNetxLS0e5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "4fxgFS2J0khy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "Mg5pIHMM0o3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "J2AaaSxI0r3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "dDO23e1k6eEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING TENSORFLOW AND KERAS"
      ],
      "metadata": {
        "id": "vQpzeVa3_iZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XdDZXX0JAG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "P8kadipN4yQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architechture of the Neural Netork"
      ],
      "metadata": {
        "id": "NLx34-NZAKS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function is Sigmoid as we are classifying into purchased and not purchased"
      ],
      "metadata": {
        "id": "1HoVzn325e68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BATCH GRADIENT DESCENT"
      ],
      "metadata": {
        "id": "ExF5bh7dAX8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10,activation='relu',input_dim=2))\n",
        "model.add(Dense(10,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "Za94CG-k9T34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE have taken 2 inputs Age and Estimated_salary\n",
        "\n",
        "\n",
        "We have 2 hidden layers of 10 neurons each with activation function ' relu ' as it works the best\n",
        "\n",
        "IN the output layer- WE have 1 neuron with activation function sigmoid because it is best for classfication\n",
        "\n"
      ],
      "metadata": {
        "id": "7KTL03vP33S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Pxbb7KDa4TUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use binary_crossentropy as we are classifying purchased into yes or no\n",
        "\n",
        "2 classes"
      ],
      "metadata": {
        "id": "z8BLtZlb5HO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',metrics=['accuracy'])  #compiling the model\n",
        "start = time.time()\n",
        "history = model.fit(X_train,y_train,epochs=10 , batch_size=320)\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "GwVATKuS5BI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOCHASTIC GRADIENT DESCENT"
      ],
      "metadata": {
        "id": "6Z8JN7c8AY0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(10,activation='relu',input_dim=2))\n",
        "model.add(Dense(10,activation='relu'))\n",
        "model.add(Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "8W9IwxDD7sFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "fTUjICGt7vB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',metrics=['accuracy'])  #compiling the model\n",
        "start = time.time()\n",
        "history = model.fit(X_train,y_train,epochs=10 , batch_size=1)\n",
        "print(time.time() - start)"
      ],
      "metadata": {
        "id": "O5DJp3E87xV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TIME TAKEN BY BGD IS 1.2 WHILE TIME TAKEN BY SGD IS 6.5.\n",
        "\n",
        "\n",
        "\n",
        "# BGD IS FASTER THAN SGD ,IT JUST GOES THROUGH ALL THE ROWS AND UPDATES ALL THE WEIGHTS / BIASES TOGETHER USING DOT PRODUCT ALL AT ONCE IN EACH EPOCH\n",
        "#AS IT PROCESSES ALL THE TOGETHER ( SO THIS WONT WORK IF OUR DATA SET IS IN MILLIONS AS WE WONT HAVE SOO MUCH RAM IN THE SYSTEM . WHERE AS SGD UPDATES WEIGHTS AND BIASES FOR EACH DAMN  ROW , SO 320 TIMES PER EPOCH MAKING IT TOO SLOW .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#BUT--\n",
        "\n",
        "\n",
        "#SGD CONVERGES FASTER TO THE MINIMA BECAUSE IN EACH EPOCH UPDATION IN SGD IS 320 TIMES WHILE FOR EACH EPOCH BGD UPDATION IS ONLY 1.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#SGD CAN ESCAPE LOCAL MINIMA EASILY COMAPRED TO BGD AS FREQUENCY OF CHANGE IS MORE ."
      ],
      "metadata": {
        "id": "dHNeLV-iC_pP"
      }
    }
  ]
}
