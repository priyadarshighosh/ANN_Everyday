{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsidLHVf5p0dy8eJK1sSCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadarshighosh/Study-DL-Everyday/blob/main/DL_Gre_prediction_DAY7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRADUATE ADMISSION PREDICTION using ANN"
      ],
      "metadata": {
        "id": "K6Veferveeh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraies for the Data Science , Machine learning and Deep Learning"
      ],
      "metadata": {
        "id": "f4UP9DvI6R91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For DATA SCIENCE"
      ],
      "metadata": {
        "id": "dC-yw5kJc6dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InPpZPIIU1Rd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For MACHINE LEARNING"
      ],
      "metadata": {
        "id": "D98SOJAzcyYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "from sklearn.pipeline import Pipeline,make_pipeline                    #here we wont use pipelines\n",
        "from sklearn.feature_selection import SelectKBest,chi2                 #feature selection                     #standard scaling\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "8yR67aMucf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For DEEP LEARNING"
      ],
      "metadata": {
        "id": "hm6sAs2dctxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf                                                   #deep learning library\n",
        "from tensorflow import keras                                              # keras integrated with tf\n",
        "\n",
        "from tensorflow.keras import Sequential                                   #sequential model - arranging the Keras layers in a sequential order\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "metadata": {
        "id": "42LofWj8cjfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Rbhi2ULX6A--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the data form the GRE DATASET"
      ],
      "metadata": {
        "id": "q9TTxc-9dEjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "_Mu4Y8BJxaCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Admission_Dataset.csv')   #fitting the data in the df dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "7YMyMH-rx0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "09Lm32E0epwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "XvnD64epesO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "D6HB7R0uesLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "hWX484aKfB6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping Unnessary Columns"
      ],
      "metadata": {
        "id": "spOtcpD3fVEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Serial No.'],inplace=True)              #dropping the serial number column as it is useless"
      ],
      "metadata": {
        "id": "BwSfiWZHffGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "R-0QEaYbkg6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN / TEST SPLIT"
      ],
      "metadata": {
        "id": "wnd26OVwkmf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Chance of Admit '])\n",
        "y = df['Chance of Admit ']"
      ],
      "metadata": {
        "id": "HSlrYtMVmrRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "XFb0LQVkmuaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "BLiA39FjmxVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "mNS4G2AYCKFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train          #TRANING SET THAT CONTAINS ALL THE OTHER FEATURES"
      ],
      "metadata": {
        "id": "nxI-Q12hk9u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train               # Training SET that contains CHANCE OF ADMIT"
      ],
      "metadata": {
        "id": "mIQ1IHyFm1N4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test                 #testing set that contains all  the features"
      ],
      "metadata": {
        "id": "TO2ZdyBsnWyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test                          #testing set that contains the test results"
      ],
      "metadata": {
        "id": "IsV6PPxCnFrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCALING MIN MAX"
      ],
      "metadata": {
        "id": "5Y7Huc-1oAZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)        #we are storing the scaled values of both the X_train\n",
        "\n",
        "X_test_scaled = scaler.fit_transform(X_test)           #and X_test"
      ],
      "metadata": {
        "id": "h2a2EiQuqKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It transformed the value of each feature between 0 to 1 so that the neural netwroks ANN making calculations on it find it easy"
      ],
      "metadata": {
        "id": "tFoDuzowq6QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ya ya it wont take your job , BUT I WILL \"HAA AH HAA HAA\""
      ],
      "metadata": {
        "id": "enKpm0pMrQUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled"
      ],
      "metadata": {
        "id": "gw5Wy1jUquzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_scaled"
      ],
      "metadata": {
        "id": "Q4sZctuRqzgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING THE NEURAL NETWORK"
      ],
      "metadata": {
        "id": "9eiiFKi5rxJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architechture of the NEURAL NETWORK"
      ],
      "metadata": {
        "id": "GSz0ofCXwtS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   7 inputs so  ( Input layer = 7 )\n",
        "*   7 Nodes ( The Hidden Layer = RELU Function ) here weight are 7x7 = 49 weights plus for each node in the hidden layer there is 1 bias so 7 more bias\n",
        "TOTAL : 49 + 7 = 56\n",
        "*   7 Nodes ( The Hidden Layer = RELU Function ) here weight are 7x7 = 49 weights plus for each node in the hidden layer there is 1 bias so 7 more bias\n",
        "TOTAL : 49 + 7 = 56\n",
        "*   1 Output layer it has ( Linear function )  here has 7 Weights and 1 Bias = 8\n",
        "\n",
        "TOTAL  (49+7) + (49+7) + (7+1) = 120\n",
        "\n"
      ],
      "metadata": {
        "id": "kHHtZIAQw7-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are working with a regression model then the output layers activation function should be linear"
      ],
      "metadata": {
        "id": "bma9Z1Q0wFLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "                                                     #It has 7 inputs in the input layer\n",
        "model.add(Dense(7,activation='relu',input_dim=7))\n",
        "\n",
        "model.add(Dense(7,activation='relu',input_dim=7))\n",
        "\n",
        "model.add(Dense(1,activation='linear'))      #output layer has an activation function that is linear with only 1 node"
      ],
      "metadata": {
        "id": "PFV6R_X9wqB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "zS5fQgeGwOyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mean_squared_error',optimizer='Adam',metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "kpKKOakbzbe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As usual we gonna use ADAM optimizer\n",
        "# Our Loss function will be mean squared error"
      ],
      "metadata": {
        "id": "1CH2DDQ6zh8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the model and Prediction"
      ],
      "metadata": {
        "id": "Airs77hAz9Ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is done on X_train_scaled because then Machine will be able to work easily"
      ],
      "metadata": {
        "id": "l7YMKaI80l0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation will be done on 20 perc of the data\n",
        "\n",
        "\n",
        "EPOCHS is a hyperparameter that determines the training process of a model. It's made up of batches and iterations, and the number of epochs can range from one to infinity."
      ],
      "metadata": {
        "id": "ZQEYSnHK0MfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " history = model.fit(X_train_scaled,y_train,epochs=150, validation_split=0.2)  #fitting the model"
      ],
      "metadata": {
        "id": "YAUyXzxt0Cn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I increased the epochs 150 so that the loss decreases accuracy increases\n",
        "Also increased the Cross validation from 20 to 30 perc SAME REASON"
      ],
      "metadata": {
        "id": "Qr6UeK151UEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].get_weights()                   #These are the 49 wights from the of the input layer to the hidden layer  and 7 biases also goes to the hidden layer for the 7 nodes"
      ],
      "metadata": {
        "id": "Z6LRPOMwRdrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION AND ACCURACY TIME!!!"
      ],
      "metadata": {
        "id": "seUkfbS817te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "TPdo86gx2Hcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_test,y_pred)   #accuracy"
      ],
      "metadata": {
        "id": "n_XDsWMs2P75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SO WE GOT AN ACCURACY OF 78 , well its not because I am a  bad programmer\n",
        "Its because DEEP LEARNING is a data hungry field where the accuracy gonna increase if the amount of data increases\n",
        "If the data is less we should use ML in instead of DL but if Data is huge like 60k images of numbers then we should use ANN or neural networks"
      ],
      "metadata": {
        "id": "lqvp7X-v3llC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LET'S GO GRAPHING!!!"
      ],
      "metadata": {
        "id": "II2JNQvX4Kv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model LOSS')\n",
        "plt.ylabel('LOSS')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')"
      ],
      "metadata": {
        "id": "mXzru59K4aKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}