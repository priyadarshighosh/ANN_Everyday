{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGIxyy7BfmbVanRRY1/J49",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyadarshighosh/Study-DL-Everyday/blob/main/DL_Customer_CHURN_Rate_DAY5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING PYTHON LIBRARIES , SCIKIT LEARN LIBRARIES AND KERAS"
      ],
      "metadata": {
        "id": "o7EyvCZQ8TYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SnPxNDsAw903"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "from sklearn.pipeline import Pipeline,make_pipeline                    #here we wont use pipelines\n",
        "from sklearn.feature_selection import SelectKBest,chi2                 #feature selection                     #standard scaling\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "FWrCDQmTFlj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()\n",
        ""
      ],
      "metadata": {
        "id": "_Mu4Y8BJxaCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Churn_Modelling.csv')   #fitting the data in the df dataframe\n",
        "df.head()\n",
        ""
      ],
      "metadata": {
        "id": "7YMyMH-rx0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "GfOzCNlnyng6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "mvfGET6LyvCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "IUCq-UNsy155"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Exited'].value_counts()"
      ],
      "metadata": {
        "id": "yYvjUMvo2jFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Geography'].value_counts()"
      ],
      "metadata": {
        "id": "aX2o0vPW2qq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender'].value_counts()"
      ],
      "metadata": {
        "id": "IduNkBMg2y6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DROPPING COLUMNS that are unnessary"
      ],
      "metadata": {
        "id": "AlUwrMHx8CfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['RowNumber','CustomerId','Surname'],inplace=True)  #we are putting in place so the date changed becomes permanent"
      ],
      "metadata": {
        "id": "tC_zCVpI3H2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5Q166OLb35-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE SHOULD BE DOING EDA NOW BUT here our aim is not doing a proper project but\n",
        "\n",
        "USE KERAS LIBRARIES to perform ANN functions"
      ],
      "metadata": {
        "id": "b5y_DXVQ3-vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONE HOT ENCODING                         ( dummy encoding )"
      ],
      "metadata": {
        "id": "kJFKc5lD7ccb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df,columns=['Geography', 'Gender'],drop_first=True)       #we get the dummy encoding for the following columns as they are norminal categories"
      ],
      "metadata": {
        "id": "iqyJyuof4Ocy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: store the above table in df\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.get_dummies(df,columns=['Geography', 'Gender'],drop_first=True)       #we get the dummy encoding for the following columns as they are norminal categories\n"
      ],
      "metadata": {
        "id": "Zel_xtW45yR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WEIGHTS ARE VERY IMPORTANT IN NEURAL NETWORKS"
      ],
      "metadata": {
        "id": "tyicZ0VN9dhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop(columns=['Exited'])\n",
        "y=df['Exited']"
      ],
      "metadata": {
        "id": "1w_x9H0XCVNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITTING THE DATA INTO TEST AND TRAINING SET SO THAT LATER WE GONNA COMPARE"
      ],
      "metadata": {
        "id": "fBm7j_QSPXND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "mNS4G2AYCKFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "sdMoRo0uCi1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "JNbggZW7CwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "JksHiOKICzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "9lBNIk3AHP9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled"
      ],
      "metadata": {
        "id": "vql1ZiM5HTQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEURAL NETWORKS using TENSORFLOW AND KERAS"
      ],
      "metadata": {
        "id": "fOynXsTfKoT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "qBviLsHJKbwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the architecture of the neural netwrok\n",
        "\n",
        "*   11 inputs ( Input Layer 11 )\n",
        "*   3 nodes ( Hidden layer -Sigmoid Function )  here 33 weights and 3 bias will be there 1 bias for eaach node of the hidden layer and 33 as in 11 input each of which goes to 3 nodes so 3x11 = 33  plus 3 bias of each node\n",
        "*   1 output layer ( Signmoid Function ) here 3 wights plus 1 bias of the node\n",
        "total 4\n",
        "\n",
        "*   Total (33+3) + (3+1) = 40\n",
        "\n"
      ],
      "metadata": {
        "id": "QmOJP__zMJeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  model = Sequential()\n",
        "  model.add(Dense(3,activation='sigmoid',input_dim=11))            #input parameter = 11\n",
        "                                                                  # hidden layer uses sigmoid function  3 nodes\n",
        "  model.add(Dense(1,activation='sigmoid'))                         #output layer uses sigmoid function  1 node"
      ],
      "metadata": {
        "id": "2s5SVHF7OtYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "p1fRyGmsOyv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])     #log loss function = binary crossentrophy\n",
        "                                                                                    #type of gradient descent algorithm"
      ],
      "metadata": {
        "id": "LpyKwJ0pPOg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled,y_train,epochs=100,verbose=1)         #epochs no of the it gonna move"
      ],
      "metadata": {
        "id": "IxOnJZhJQ_Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUR is at 0.39"
      ],
      "metadata": {
        "id": "-e0w57GURVMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].get_weights()                   #These are the 33 wights from the of the input layer to the hidden layer  and 3 biases also goes to the hidden layer for the 3 nodes"
      ],
      "metadata": {
        "id": "Z6LRPOMwRdrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[1].get_weights()                #these are 3 weights from the hidden layer to the output layyer with 1 bais of the output layer ka node"
      ],
      "metadata": {
        "id": "Gqouuz7jXDuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now prediction TIME"
      ],
      "metadata": {
        "id": "6eubU2DTXgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)            #we using sigmoid so it gives a number between 0 and 1"
      ],
      "metadata": {
        "id": "MSHUD3zaXkEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_log=model.predict(X_test_scaled)\n",
        "\n",
        "np.where(y_log>0.5,1,0)                 # if the sigmoid function gives more than 0.5 then it is stated as 1"
      ],
      "metadata": {
        "id": "ExaLte4AXy3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACCURACY TIME"
      ],
      "metadata": {
        "id": "ecOpEzAEYH_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.where(y_log>0.5,1,0)"
      ],
      "metadata": {
        "id": "7m2PfL-dYUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "AtUoNWfeYNOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "WE can increase the accuracy\n",
        "*   Increasing the no of epochs\n",
        "*   Changing the activation function\n",
        "*   Increasing the no of nodes\n",
        "*   Increasing the no of layers                     #but too many nodes and layers maylead to overfitting\n"
      ],
      "metadata": {
        "id": "lgWa_W2SYvLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we getting a 85.3 perc accuracy"
      ],
      "metadata": {
        "id": "vMVDnE2TYfRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}