{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING PYTHON LIBRARIES , SCIKIT LEARN LIBRARIES AND KERAS"
      ],
      "metadata": {
        "id": "o7EyvCZQ8TYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "SnPxNDsAw903"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split               #for splitting the data into test and training data\n",
        "from sklearn.compose import ColumnTransformer                       #for transforming the columns\n",
        "from sklearn.impute import SimpleImputer                             #for imputing the missing values\n",
        "from sklearn.preprocessing import OneHotEncoder                      #one hot encoding\n",
        "from sklearn.preprocessing import MinMaxScaler                        #standard scaling\n",
        "from sklearn.pipeline import Pipeline,make_pipeline                    #here we wont use pipelines\n",
        "from sklearn.feature_selection import SelectKBest,chi2                 #feature selection                     #standard scaling\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "FWrCDQmTFlj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files    # we are importing the file from the device\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "_Mu4Y8BJxaCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('Churn-Dataset.csv')   #fitting the data in the df dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "7YMyMH-rx0ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "GfOzCNlnyng6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n"
      ],
      "metadata": {
        "id": "mvfGET6LyvCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "IUCq-UNsy155"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Exited'].value_counts()"
      ],
      "metadata": {
        "id": "yYvjUMvo2jFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Geography'].value_counts()"
      ],
      "metadata": {
        "id": "aX2o0vPW2qq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Gender'].value_counts()"
      ],
      "metadata": {
        "id": "IduNkBMg2y6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DROPPING COLUMNS that are unnessary"
      ],
      "metadata": {
        "id": "AlUwrMHx8CfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['RowNumber','CustomerId','Surname'],inplace=True)  #we are putting in place so the date changed becomes permanent"
      ],
      "metadata": {
        "id": "tC_zCVpI3H2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5Q166OLb35-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE SHOULD BE DOING EDA NOW BUT here our aim is not doing a proper project but\n",
        "\n",
        "USE KERAS LIBRARIES to perform ANN functions"
      ],
      "metadata": {
        "id": "b5y_DXVQ3-vO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONE HOT ENCODING                         ( dummy encoding )"
      ],
      "metadata": {
        "id": "kJFKc5lD7ccb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df,columns=['Geography', 'Gender'],drop_first=True)       #we get the dummy encoding for the following columns as they are norminal categories"
      ],
      "metadata": {
        "id": "iqyJyuof4Ocy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: store the above table in df\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.get_dummies(df,columns=['Geography', 'Gender'],drop_first=True)       #we get the dummy encoding for the following columns as they are norminal categories\n"
      ],
      "metadata": {
        "id": "Zel_xtW45yR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WEIGHTS ARE VERY IMPORTANT IN NEURAL NETWORKS"
      ],
      "metadata": {
        "id": "tyicZ0VN9dhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop(columns=['Exited'])\n",
        "y=df['Exited']"
      ],
      "metadata": {
        "id": "1w_x9H0XCVNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITTING THE DATA INTO TEST AND TRAINING SET SO THAT LATER WE GONNA COMPARE"
      ],
      "metadata": {
        "id": "fBm7j_QSPXND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
      ],
      "metadata": {
        "id": "mNS4G2AYCKFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "sdMoRo0uCi1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "JNbggZW7CwYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "JksHiOKICzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "9lBNIk3AHP9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled"
      ],
      "metadata": {
        "id": "vql1ZiM5HTQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEURAL NETWORKS using TENSORFLOW AND KERAS"
      ],
      "metadata": {
        "id": "fOynXsTfKoT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "qBviLsHJKbwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THIS IS THE IMPORVED VERSION WITH MORE ACCURACY"
      ],
      "metadata": {
        "id": "MJbB2hNdnRlC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the architecture of the neural netwrok\n",
        "\n",
        "*   11 inputs ( Input Layer 11 )\n",
        "*   11 nodes ( Hidden layer #1 -RELU Function )  here 121 weights and 11 bias will be there 1 bias for eaach node of the hidden layer and 121 as in 11 input each of which goes to 11 nodes so 11x11 = 121  plus 11 bias of each node\n",
        "*   11 nodes (Hidden layer #2 -RELU Function ) here 121 weights and 11 bias\n",
        "will be there 1 bias for each node of the hidden layer and 121 as in 11inout each of which goes to 111 nodes so 11x11= 121 plus 11 bias of each node\n",
        "*   1 output layer ( Signmoid Function ) here 11 wights plus 1 bias of the node\n",
        "total 12\n",
        "\n",
        "*   Total (121+11) + (121+11) + (11+1) =  278\n",
        "\n"
      ],
      "metadata": {
        "id": "QmOJP__zMJeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  model = Sequential()\n",
        "  model.add(Dense(11,activation='relu',input_dim=11))            #input parameter = 11\n",
        "                                                                  # hidden layer1 uses relu  function  11 nodes\n",
        "  model.add(Dense(11, activation ='relu'))                        #hidden layer2 uses relu function  11 nodes\n",
        "  model.add(Dense(1,activation='sigmoid'))                         #output layer uses sigmoid function  1 node"
      ],
      "metadata": {
        "id": "2s5SVHF7OtYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "p1fRyGmsOyv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])     #log loss function = binary crossentrophy\n",
        "                                                                                    #type of gradient descent algorithm"
      ],
      "metadata": {
        "id": "LpyKwJ0pPOg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CROSS VALIDATION     will be done on 20perc of training data"
      ],
      "metadata": {
        "id": "XYBXOZ6Wq4f1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_scaled,y_train,epochs=300,validation_split= 0.2)         #epochs no of the it gonna move\n",
        "                                                                                     #to draw graph we storing the information in history"
      ],
      "metadata": {
        "id": "IxOnJZhJQ_Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE GOT 4 parameter Validation loss  , Validation accuracy , actual accuracy and actual loss\n",
        "\n",
        "The actual accuracy should not be very different from loss accuracy it would mean that the there is chance for over-fitting"
      ],
      "metadata": {
        "id": "jhFXGoA9sYhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*OUR*  loss is at 0.30"
      ],
      "metadata": {
        "id": "-e0w57GURVMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# history keyword this gives 4 parameters\n",
        "\n",
        "\n",
        "*   Loss\n",
        "*   Accuracy\n",
        "*   Val-Loss\n",
        "*   Val-Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "413783hQuyrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history.history"
      ],
      "metadata": {
        "id": "ZAM_o_MPt4r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].get_weights()                   #These are the 33 wights from the of the input layer to the hidden layer  and 3 biases also goes to the hidden layer for the 3 nodes"
      ],
      "metadata": {
        "id": "Z6LRPOMwRdrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[1].get_weights()                #these are 3 weights from the hidden layer to the output layyer with 1 bais of the output layer ka node"
      ],
      "metadata": {
        "id": "Gqouuz7jXDuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now prediction TIME"
      ],
      "metadata": {
        "id": "6eubU2DTXgA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X_test_scaled)            #we using sigmoid so it gives a number between 0 and 1"
      ],
      "metadata": {
        "id": "MSHUD3zaXkEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_log=model.predict(X_test_scaled)\n",
        "\n",
        "np.where(y_log>0.5,1,0)                 # if the sigmoid function gives more than 0.5 then it is stated as 1"
      ],
      "metadata": {
        "id": "ExaLte4AXy3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACCURACY TIME"
      ],
      "metadata": {
        "id": "ecOpEzAEYH_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.where(y_log>0.5,1,0)"
      ],
      "metadata": {
        "id": "7m2PfL-dYUoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "AtUoNWfeYNOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "WE can increase the accuracy\n",
        "*   Increasing the no of epochs\n",
        "*   Changing the activation function\n",
        "*   Increasing the no of nodes\n",
        "*   Increasing the no of layers                     #but too many nodes and layers maylead to overfitting\n"
      ],
      "metadata": {
        "id": "lgWa_W2SYvLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we getting a 87 perc accuracy"
      ],
      "metadata": {
        "id": "vMVDnE2TYfRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the 2 graphs for Accuracy and LOSS"
      ],
      "metadata": {
        "id": "2oWHDgiay1ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='center right')\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "id": "mt-FBi-FvpIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='center right')"
      ],
      "metadata": {
        "id": "dk7xJS_1wh7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WE GONNA LATER USE REGULARIZATION AND other stuff to make the acccuracy more"
      ],
      "metadata": {
        "id": "XlUiJ1xHxGIK"
      }
    }
  ]
}